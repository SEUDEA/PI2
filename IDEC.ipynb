{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bf887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cde9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174fc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((368,368)),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e39c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, latent_dim, 3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z.view(z.size(0), -1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        B = z.size(0)\n",
    "        z = z.view(B, self.latent_dim, 23, 23)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816256f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / torch.sum(q, dim=0)\n",
    "    return (weight.t() / torch.sum(weight, dim=1)).t()\n",
    "\n",
    "\n",
    "class IDECModel(nn.Module): # DCNModel\n",
    "    def __init__(self, autoencoder, cluster_centers, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.cluster_centers = nn.Parameter(\n",
    "            torch.tensor(cluster_centers, dtype=torch.float32)\n",
    "        )\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.autoencoder.encode(x)\n",
    "        dist = torch.sum((z.unsqueeze(1) - self.cluster_centers)**2, dim=2)\n",
    "        q = 1.0 / (1.0 + dist / self.alpha)\n",
    "        q = q ** ((self.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
    "        x_rec = self.autoencoder.decode(z)\n",
    "        return q, x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8852a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataset, device):\n",
    "    print(\"[AE] Iniciando pre-entrenamiento del Autoencoder\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.NAdam(model.parameters(), weight_decay=1e-5, lr=AE_LEARNING_RATE) # TODO: Nadam https://keras.io/api/optimizers/Nadam/\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=AE_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    print(f\"[AE] Entrenando Autoencoder por {AE_EPOCHS} épocas...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(AE_EPOCHS):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for data in loader:\n",
    "            inputs = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        avg_epoch_loss = epoch_loss / len(dataset)\n",
    "        print(f\"\\t[AE] Época {epoch+1} completada — loss promedio: {avg_epoch_loss:.6f}\")  \n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[AE] Entrenamiento completado en {end_time - start_time:.2f} segundos.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435861dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_joint_clustering(dataset):\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\\n\")\n",
    "\n",
    "    autoencoder = Autoencoder(ENCODING_DIM)\n",
    "    autoencoder = train_autoencoder(autoencoder, dataset, device)\n",
    "\n",
    "    autoencoder.to(device)\n",
    "    autoencoder.eval()\n",
    "\n",
    "    print(\"Extrayendo representaciones latentes para Clustering…\")  \n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(dataset, batch_size=AE_BATCH_SIZE, shuffle=False):\n",
    "            x = batch.to(device)\n",
    "            z = autoencoder.encode(x)\n",
    "            latents.append(z.cpu())\n",
    "\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "    print(f\"Latents extraídas: shape = {latents.shape}\\n\")  \n",
    "\n",
    "    print(f\"Inicializando KMeans con {NUM_CLUSTERS} clusters…\")  \n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=RANDOM_STATE, n_init=30, init=\"k-means++\") # TODO: cambiar por KMeans+++ o mini bach KMeans\n",
    "    y_pred_kmeans = kmeans.fit_predict(latents)\n",
    "    print(\"KMeans completado. Centroides iniciales obtenidos.\\n\") \n",
    "\n",
    "    cluster_centers_init = kmeans.cluster_centers_\n",
    "    y_pred_last = y_pred_kmeans\n",
    "\n",
    "    # print(f\"Inicializando AgglomerativeClustering con {NUM_CLUSTERS} clusters…\")\n",
    "    # agg = AgglomerativeClustering(n_clusters=NUM_CLUSTERS, metric=\"cosine\", linkage=\"average\")\n",
    "    # y_pred_agg = agg.fit_predict(latents)\n",
    "    # print(\"AgglomerativeClustering completado. Etiquetas obtenidas.\\n\")\n",
    "\n",
    "    # cluster_centers_init = np.vstack([\n",
    "    #     latents[y_pred_agg == j].mean(axis=0)\n",
    "    #     for j in range(NUM_CLUSTERS)\n",
    "    # ])\n",
    "    # y_pred_last = y_pred_agg\n",
    "\n",
    "    # print(f\"Inicializando SpectralClustering con {NUM_CLUSTERS} clusters…\")\n",
    "    # spec = SpectralClustering(\n",
    "    #     n_clusters=NUM_CLUSTERS,\n",
    "    #     affinity='nearest_neighbors',\n",
    "    #     n_neighbors=7,\n",
    "    #     assign_labels='kmeans',\n",
    "    #     eigen_solver='arpack',\n",
    "    #     n_init=10,\n",
    "    #     random_state=RANDOM_STATE,\n",
    "    # )\n",
    "    # y_pred_spec = spec.fit_predict(latents)\n",
    "    # print(\"SpectralClustering completado. Etiquetas obtenidas.\\n\")\n",
    "\n",
    "    # cluster_centers_init = np.vstack([\n",
    "    #     latents[y_pred_spec == j].mean(axis=0)\n",
    "    #     for j in range(NUM_CLUSTERS)\n",
    "    # ])\n",
    "    # y_pred_last = y_pred_spec\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    idec_model = IDECModel(autoencoder, cluster_centers_init, alpha=1.0)\n",
    "    idec_model.to(device)\n",
    "\n",
    "    optimizer = optim.NAdam(idec_model.parameters(), weight_decay=1e-5, lr=AE_LEARNING_RATE)\n",
    "\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    kl_criterion    = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    p_target = None\n",
    "\n",
    "    print(\"[Joint] Entrenamiento conjunto arrancando…\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(JOINT_EPOCHS):\n",
    "        if epoch % UPDATE_INTERVAL == 0:\n",
    "            print(f\"\\t[Joint] Actualizando p_target (epoch % {UPDATE_INTERVAL} == 0)\")\n",
    "\n",
    "            all_q = []\n",
    "            with torch.no_grad():\n",
    "                for batch in DataLoader(dataset, batch_size=JOINT_BATCH_SIZE, shuffle=False):\n",
    "                    batch = batch.to(device)\n",
    "                    q_batch, _ = idec_model(batch)\n",
    "                    all_q.append(q_batch)\n",
    "            q_all = torch.cat(all_q, dim=0)\n",
    "            p_target = target_distribution(q_all)\n",
    "\n",
    "            y_pred_current = torch.argmax(q_all, dim=1).cpu().numpy()\n",
    "\n",
    "            delta = np.mean(y_pred_current != y_pred_last)\n",
    "            if epoch > 0 and delta < TOL:\n",
    "                print(f\"[Joint] Convergencia alcanzada (Δ={delta:.4f} < {TOL}). Deteniendo entrenamiento.\")\n",
    "                break\n",
    "            y_pred_last = y_pred_current\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(DataLoader(dataset, batch_size=JOINT_BATCH_SIZE, shuffle=True)):\n",
    "            batch = batch.to(device)\n",
    "            q_batch, x_rec = idec_model(batch)\n",
    "\n",
    "            if p_target is not None:\n",
    "                start = batch_idx * JOINT_BATCH_SIZE\n",
    "                end   = start + batch.size(0)\n",
    "                p_batch = p_target[start:end].to(device)\n",
    "            else:\n",
    "                p_batch = q_batch.detach()\n",
    "\n",
    "            loss_kl = kl_criterion(q_batch.log(), p_batch) # TODO: Revisar la ecuación directamente\n",
    "            loss_recon = recon_criterion(x_rec, batch) # TODO: Revisar la ecuación directamente\n",
    "\n",
    "            loss = GAMMA_IMAGES * loss_kl + loss_recon\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        avg_joint_loss = epoch_loss / len(dataset)\n",
    "        print(f\"\\t[Joint] Época {epoch+1} finalizada — loss promedio: {avg_joint_loss:.6f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[Joint] Entrenamiento completado en {end_time - start_time:.2f} segundos.\")\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Obteniendo predicciones finales…\")     \n",
    "    idec_model.eval()\n",
    "\n",
    "    all_q = []\n",
    "    new_latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(dataset, batch_size=JOINT_BATCH_SIZE, shuffle=False):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            z = idec_model.autoencoder.encode(batch)\n",
    "            new_latents.append(z.cpu())\n",
    "\n",
    "            q_batch, _ = idec_model(batch)\n",
    "            all_q.append(q_batch)\n",
    "\n",
    "    q_final = torch.cat(all_q, dim=0)\n",
    "    y_pred_final = torch.argmax(q_final, dim=1).cpu().numpy()\n",
    "    new_latents = torch.cat(new_latents, dim=0).numpy()\n",
    "    print(\"Predicciones finales obtenidas.\\n\")\n",
    "\n",
    "    return new_latents, y_pred_final, idec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfa510",
   "metadata": {},
   "source": [
    "# Dataset Boold Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73798aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "[AE] Iniciando pre-entrenamiento del Autoencoder\n",
      "[AE] Entrenando Autoencoder por 30 épocas...\n",
      "\t[AE] Época 1 completada — loss promedio: 0.052959\n",
      "\t[AE] Época 2 completada — loss promedio: 0.016883\n",
      "\t[AE] Época 3 completada — loss promedio: 0.009705\n",
      "\t[AE] Época 4 completada — loss promedio: 0.007372\n",
      "\t[AE] Época 5 completada — loss promedio: 0.006744\n",
      "\t[AE] Época 6 completada — loss promedio: 0.006210\n",
      "\t[AE] Época 7 completada — loss promedio: 0.006061\n",
      "\t[AE] Época 8 completada — loss promedio: 0.005790\n",
      "\t[AE] Época 9 completada — loss promedio: 0.005667\n",
      "\t[AE] Época 10 completada — loss promedio: 0.005452\n",
      "\t[AE] Época 11 completada — loss promedio: 0.005247\n",
      "\t[AE] Época 12 completada — loss promedio: 0.005135\n",
      "\t[AE] Época 13 completada — loss promedio: 0.005028\n",
      "\t[AE] Época 14 completada — loss promedio: 0.004853\n",
      "\t[AE] Época 15 completada — loss promedio: 0.004664\n",
      "\t[AE] Época 16 completada — loss promedio: 0.004603\n",
      "\t[AE] Época 17 completada — loss promedio: 0.004449\n",
      "\t[AE] Época 18 completada — loss promedio: 0.004296\n",
      "\t[AE] Época 19 completada — loss promedio: 0.004184\n",
      "\t[AE] Época 20 completada — loss promedio: 0.003996\n",
      "\t[AE] Época 21 completada — loss promedio: 0.024897\n",
      "\t[AE] Época 22 completada — loss promedio: 0.005450\n",
      "\t[AE] Época 23 completada — loss promedio: 0.004704\n",
      "\t[AE] Época 24 completada — loss promedio: 0.004325\n",
      "\t[AE] Época 25 completada — loss promedio: 0.004108\n",
      "\t[AE] Época 26 completada — loss promedio: 0.003987\n",
      "\t[AE] Época 27 completada — loss promedio: 0.003849\n",
      "\t[AE] Época 28 completada — loss promedio: 0.003797\n",
      "\t[AE] Época 29 completada — loss promedio: 0.003716\n",
      "\t[AE] Época 30 completada — loss promedio: 0.003637\n",
      "[AE] Entrenamiento completado en 727.20 segundos.\n",
      "Extrayendo representaciones latentes para Clustering…\n",
      "Latents extraídas: shape = (5000, 67712)\n",
      "\n",
      "Inicializando SpectralClustering con 5 clusters…\n",
      "SpectralClustering completado. Etiquetas obtenidas.\n",
      "\n",
      "[Joint] Entrenamiento conjunto arrancando…\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 1 finalizada — loss promedio: 0.022941\n",
      "\t[Joint] Época 2 finalizada — loss promedio: 0.022470\n",
      "\t[Joint] Época 3 finalizada — loss promedio: 0.022378\n",
      "\t[Joint] Época 4 finalizada — loss promedio: 0.022365\n",
      "\t[Joint] Época 5 finalizada — loss promedio: 0.022230\n",
      "\t[Joint] Época 6 finalizada — loss promedio: 0.022120\n",
      "\t[Joint] Época 7 finalizada — loss promedio: 0.022085\n",
      "\t[Joint] Época 8 finalizada — loss promedio: 0.022069\n",
      "\t[Joint] Época 9 finalizada — loss promedio: 0.022038\n",
      "\t[Joint] Época 10 finalizada — loss promedio: 0.022013\n",
      "\t[Joint] Época 11 finalizada — loss promedio: 0.021894\n",
      "\t[Joint] Época 12 finalizada — loss promedio: 0.021936\n",
      "\t[Joint] Época 13 finalizada — loss promedio: 0.021733\n",
      "\t[Joint] Época 14 finalizada — loss promedio: 0.021839\n",
      "\t[Joint] Época 15 finalizada — loss promedio: 0.021706\n",
      "\t[Joint] Época 16 finalizada — loss promedio: 0.021691\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 17 finalizada — loss promedio: 0.030681\n",
      "\t[Joint] Época 18 finalizada — loss promedio: 0.008597\n",
      "\t[Joint] Época 19 finalizada — loss promedio: 0.007210\n",
      "\t[Joint] Época 20 finalizada — loss promedio: 0.006696\n",
      "\t[Joint] Época 21 finalizada — loss promedio: 0.006405\n",
      "\t[Joint] Época 22 finalizada — loss promedio: 0.006248\n",
      "\t[Joint] Época 23 finalizada — loss promedio: 0.006094\n",
      "\t[Joint] Época 24 finalizada — loss promedio: 0.006041\n",
      "\t[Joint] Época 25 finalizada — loss promedio: 0.005968\n",
      "\t[Joint] Época 26 finalizada — loss promedio: 0.005900\n",
      "\t[Joint] Época 27 finalizada — loss promedio: 0.005875\n",
      "\t[Joint] Época 28 finalizada — loss promedio: 0.005822\n",
      "\t[Joint] Época 29 finalizada — loss promedio: 0.005839\n",
      "\t[Joint] Época 30 finalizada — loss promedio: 0.005778\n",
      "\t[Joint] Época 31 finalizada — loss promedio: 0.005773\n",
      "\t[Joint] Época 32 finalizada — loss promedio: 0.005718\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 33 finalizada — loss promedio: 0.004169\n",
      "\t[Joint] Época 34 finalizada — loss promedio: 0.004115\n",
      "\t[Joint] Época 35 finalizada — loss promedio: 0.004112\n",
      "\t[Joint] Época 36 finalizada — loss promedio: 0.004067\n",
      "\t[Joint] Época 37 finalizada — loss promedio: 0.004041\n",
      "\t[Joint] Época 38 finalizada — loss promedio: 0.004070\n",
      "\t[Joint] Época 39 finalizada — loss promedio: 0.004038\n",
      "\t[Joint] Época 40 finalizada — loss promedio: 0.004007\n",
      "\t[Joint] Época 41 finalizada — loss promedio: 0.004001\n",
      "\t[Joint] Época 42 finalizada — loss promedio: 0.003986\n",
      "\t[Joint] Época 43 finalizada — loss promedio: 0.003997\n",
      "\t[Joint] Época 44 finalizada — loss promedio: 0.003948\n",
      "\t[Joint] Época 45 finalizada — loss promedio: 0.003946\n",
      "\t[Joint] Época 46 finalizada — loss promedio: 0.003945\n",
      "\t[Joint] Época 47 finalizada — loss promedio: 0.003937\n",
      "\t[Joint] Época 48 finalizada — loss promedio: 0.003906\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 49 finalizada — loss promedio: 0.003350\n",
      "\t[Joint] Época 50 finalizada — loss promedio: 0.003321\n",
      "\t[Joint] Época 51 finalizada — loss promedio: 0.003298\n",
      "\t[Joint] Época 52 finalizada — loss promedio: 0.003270\n",
      "\t[Joint] Época 53 finalizada — loss promedio: 0.003247\n",
      "\t[Joint] Época 54 finalizada — loss promedio: 0.003206\n",
      "\t[Joint] Época 55 finalizada — loss promedio: 0.003200\n",
      "\t[Joint] Época 56 finalizada — loss promedio: 0.003152\n",
      "\t[Joint] Época 57 finalizada — loss promedio: 0.003133\n",
      "\t[Joint] Época 58 finalizada — loss promedio: 0.003115\n",
      "\t[Joint] Época 59 finalizada — loss promedio: 0.003088\n",
      "\t[Joint] Época 60 finalizada — loss promedio: 0.003054\n",
      "\t[Joint] Época 61 finalizada — loss promedio: 0.003032\n",
      "\t[Joint] Época 62 finalizada — loss promedio: 0.003014\n",
      "\t[Joint] Época 63 finalizada — loss promedio: 0.002992\n",
      "\t[Joint] Época 64 finalizada — loss promedio: 0.002953\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 65 finalizada — loss promedio: 0.002690\n",
      "\t[Joint] Época 66 finalizada — loss promedio: 0.002665\n",
      "\t[Joint] Época 67 finalizada — loss promedio: 0.002644\n",
      "\t[Joint] Época 68 finalizada — loss promedio: 0.002622\n",
      "\t[Joint] Época 69 finalizada — loss promedio: 0.002605\n",
      "\t[Joint] Época 70 finalizada — loss promedio: 0.002565\n",
      "\t[Joint] Época 71 finalizada — loss promedio: 0.002550\n",
      "\t[Joint] Época 72 finalizada — loss promedio: 0.002510\n",
      "\t[Joint] Época 73 finalizada — loss promedio: 0.002493\n",
      "\t[Joint] Época 74 finalizada — loss promedio: 0.002465\n",
      "\t[Joint] Época 75 finalizada — loss promedio: 0.002441\n",
      "\t[Joint] Época 76 finalizada — loss promedio: 0.002407\n",
      "\t[Joint] Época 77 finalizada — loss promedio: 0.002419\n",
      "\t[Joint] Época 78 finalizada — loss promedio: 0.002365\n",
      "\t[Joint] Época 79 finalizada — loss promedio: 0.002366\n",
      "\t[Joint] Época 80 finalizada — loss promedio: 0.002344\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 81 finalizada — loss promedio: 0.002221\n",
      "\t[Joint] Época 82 finalizada — loss promedio: 0.002185\n",
      "\t[Joint] Época 83 finalizada — loss promedio: 0.002172\n",
      "\t[Joint] Época 84 finalizada — loss promedio: 0.002157\n",
      "\t[Joint] Época 85 finalizada — loss promedio: 0.002114\n",
      "\t[Joint] Época 86 finalizada — loss promedio: 0.002146\n",
      "\t[Joint] Época 87 finalizada — loss promedio: 0.002110\n",
      "\t[Joint] Época 88 finalizada — loss promedio: 0.002083\n",
      "\t[Joint] Época 89 finalizada — loss promedio: 0.002082\n",
      "\t[Joint] Época 90 finalizada — loss promedio: 0.002052\n",
      "\t[Joint] Época 91 finalizada — loss promedio: 0.002064\n",
      "\t[Joint] Época 92 finalizada — loss promedio: 0.002045\n",
      "\t[Joint] Época 93 finalizada — loss promedio: 0.002054\n",
      "\t[Joint] Época 94 finalizada — loss promedio: 0.002024\n",
      "\t[Joint] Época 95 finalizada — loss promedio: 0.002020\n",
      "\t[Joint] Época 96 finalizada — loss promedio: 0.002030\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "\t[Joint] Época 97 finalizada — loss promedio: 0.001948\n",
      "\t[Joint] Época 98 finalizada — loss promedio: 0.001932\n",
      "\t[Joint] Época 99 finalizada — loss promedio: 0.001942\n",
      "\t[Joint] Época 100 finalizada — loss promedio: 0.001939\n",
      "\t[Joint] Época 101 finalizada — loss promedio: 0.001948\n",
      "\t[Joint] Época 102 finalizada — loss promedio: 0.001913\n",
      "\t[Joint] Época 103 finalizada — loss promedio: 0.001923\n",
      "\t[Joint] Época 104 finalizada — loss promedio: 0.001931\n",
      "\t[Joint] Época 105 finalizada — loss promedio: 0.001909\n",
      "\t[Joint] Época 106 finalizada — loss promedio: 0.001916\n",
      "\t[Joint] Época 107 finalizada — loss promedio: 0.001889\n",
      "\t[Joint] Época 108 finalizada — loss promedio: 0.001891\n",
      "\t[Joint] Época 109 finalizada — loss promedio: 0.001861\n",
      "\t[Joint] Época 110 finalizada — loss promedio: 0.001869\n",
      "\t[Joint] Época 111 finalizada — loss promedio: 0.001861\n",
      "\t[Joint] Época 112 finalizada — loss promedio: 0.001839\n",
      "\t[Joint] Actualizando p_target (epoch % 16 == 0)\n",
      "[Joint] Convergencia alcanzada (Δ=0.0028 < 0.01). Deteniendo entrenamiento.\n",
      "[Joint] Entrenamiento completado en 2901.71 segundos.\n",
      "Obteniendo predicciones finales…\n",
      "Predicciones finales obtenidas.\n",
      "\n",
      "\n",
      "--- Final Image Clustering Results ---\n",
      "Predicted cluster assignments shape: (5000,)\n",
      "Image: BA_145728.jpg, Cluster ID: 1\n",
      "Image: BA_34424.jpg, Cluster ID: 1\n",
      "Image: MYO_1397.jpg, Cluster ID: 1\n",
      "Image: ERB_506873.jpg, Cluster ID: 1\n",
      "Image: BA_363451.jpg, Cluster ID: 1\n",
      "Image: MO_998662.jpg, Cluster ID: 1\n",
      "Image: MYO_0362.jpg, Cluster ID: 1\n",
      "Image: ERB_312715.jpg, Cluster ID: 1\n",
      "Image: MYO_0315.jpg, Cluster ID: 1\n",
      "Image: MYO_1225.jpg, Cluster ID: 1\n",
      "...\n",
      "\n",
      "Cluster distribution:\n",
      "Cluster 1: 5000 images\n",
      "Insuficientes clusters para métricas.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = \"./storage/clean/blood_cell/segmenter\"\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "NUM_CLUSTERS = 5\n",
    "\n",
    "AE_EPOCHS = 30\n",
    "AE_BATCH_SIZE = 64\n",
    "AE_LEARNING_RATE = 1e-3\n",
    "\n",
    "JOINT_EPOCHS = 200\n",
    "JOINT_BATCH_SIZE = 64\n",
    "\n",
    "GAMMA_IMAGES = 0.05\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TOL = 1e-2\n",
    "UPDATE_INTERVAL = 16\n",
    "\n",
    "ENCODING_DIM = 128\n",
    "\n",
    "INPUT_DIM = 368 * 368 * 3\n",
    "\n",
    "\n",
    "dataset = LazyDataset(image_dir=IMAGE_DIR, transform=get_transform())\n",
    "image_filenames = dataset.image_filenames\n",
    "\n",
    "latents_final, cluster_labels, trained_idec_model_images = train_deep_joint_clustering(dataset)\n",
    "\n",
    "print(\"\\n--- Final Image Clustering Results ---\")\n",
    "print(f\"Predicted cluster assignments shape: {cluster_labels.shape}\")\n",
    "\n",
    "for i in range(min(10, len(image_filenames))):\n",
    "    print(f\"Image: {image_filenames[i]}, Cluster ID: {cluster_labels[i]}\")\n",
    "if len(image_filenames) > 10:\n",
    "    print(\"...\")\n",
    "\n",
    "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster_id, count in zip(unique_clusters, counts):\n",
    "    print(f\"Cluster {cluster_id}: {count} images\")\n",
    "\n",
    "\n",
    "if len(set(cluster_labels)) > 1 and len(cluster_labels) > NUM_CLUSTERS:\n",
    "    try:\n",
    "        score = silhouette_score(latents_final, cluster_labels)\n",
    "        print(f\"\\nSilhouette Score: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error Silhouette: {e}\")\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(latents_final, cluster_labels)\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error CH Index: {e}\")\n",
    "else:\n",
    "    print(\"Insuficientes clusters para métricas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"./storage/raw/cifar/cifar-10-batches-py\"\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "NUM_CLUSTERS = 5\n",
    "\n",
    "AE_EPOCHS = 30\n",
    "AE_BATCH_SIZE = 64\n",
    "AE_LEARNING_RATE = 1e-3\n",
    "\n",
    "JOINT_EPOCHS = 200\n",
    "JOINT_BATCH_SIZE = 64\n",
    "\n",
    "GAMMA_IMAGES = 0.05\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TOL = 1e-2\n",
    "UPDATE_INTERVAL = 16\n",
    "\n",
    "ENCODING_DIM = 128\n",
    "\n",
    "INPUT_DIM = 368 * 368 * 3\n",
    "\n",
    "\n",
    "dataset = LazyDataset(image_dir=IMAGE_DIR, transform=get_transform())\n",
    "image_filenames = dataset.image_filenames\n",
    "\n",
    "latents_final, cluster_labels, trained_idec_model_images = train_deep_joint_clustering(dataset)\n",
    "\n",
    "print(\"\\n--- Final Image Clustering Results ---\")\n",
    "print(f\"Predicted cluster assignments shape: {cluster_labels.shape}\")\n",
    "\n",
    "for i in range(min(10, len(image_filenames))):\n",
    "    print(f\"Image: {image_filenames[i]}, Cluster ID: {cluster_labels[i]}\")\n",
    "if len(image_filenames) > 10:\n",
    "    print(\"...\")\n",
    "\n",
    "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster_id, count in zip(unique_clusters, counts):\n",
    "    print(f\"Cluster {cluster_id}: {count} images\")\n",
    "\n",
    "\n",
    "if len(set(cluster_labels)) > 1 and len(cluster_labels) > NUM_CLUSTERS:\n",
    "    try:\n",
    "        score = silhouette_score(latents_final, cluster_labels)\n",
    "        print(f\"\\nSilhouette Score: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error Silhouette: {e}\")\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(latents_final, cluster_labels)\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error CH Index: {e}\")\n",
    "else:\n",
    "    print(\"Insuficientes clusters para métricas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
