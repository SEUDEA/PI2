{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "!!! WARNING: Using MLP Autoencoder for 360x360 images is computationally !!!\n",
      "!!! intensive and likely suboptimal. Consider using a Convolutional      !!!\n",
      "!!! Autoencoder (CAE) for better results and efficiency with images.     !!!\n",
      "!!! You may encounter memory issues or very long training times.         !!!\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "Loading images from: ./storage/clean/blood_cell/segmenter\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 273\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Load and preprocess your images\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m X_images_flattened, image_filenames, input_dim_images = \u001b[43mload_and_preprocess_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET_IMAGE_SIZE\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_images_flattened \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting deep joint clustering for image data with actual input_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mload_and_preprocess_images\u001b[39m\u001b[34m(image_dir, target_size)\u001b[39m\n\u001b[32m    200\u001b[39m img = Image.open(img_path)\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Convert to RGB (handles grayscale, RGBA, etc.)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Resize\u001b[39;00m\n\u001b[32m    206\u001b[39m img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jupyter/lib/python3.12/site-packages/PIL/Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jupyter/lib/python3.12/site-packages/PIL/ImageFile.py:280\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         s = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct.error) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[32m    283\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jupyter/lib/python3.12/site-packages/PIL/JpegImagePlugin.py:414\u001b[39m, in \u001b[36mJpegImageFile.load_read\u001b[39m\u001b[34m(self, read_bytes)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    internal: read more image data\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile.LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_ended\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    417\u001b[39m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28mself\u001b[39m._ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# --- 1. Model Definition ---\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "    It computes the Student's t-distribution similarity between input samples and cluster centroids.\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` representing initial cluster centers.\n",
    "        alpha: degrees of freedom of the Student's t-distribution (typically 1.0).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = Layer.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = Layer.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Normalize\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters, 'alpha': self.alpha}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def build_autoencoder(input_dim, latent_dim, hidden_dims=[500, 500, 2000]):\n",
    "    \"\"\"Builds a simple autoencoder model.\"\"\"\n",
    "    input_layer = Input(shape=(input_dim,), name='input')\n",
    "    h = input_layer\n",
    "    # Encoder\n",
    "    for i, dim in enumerate(hidden_dims):\n",
    "        h = Dense(dim, activation='relu', name=f'encoder_hidden_{i}')(h)\n",
    "    latent = Dense(latent_dim, name='latent_space')(h) # Linear activation for latent space\n",
    "    encoder = Model(inputs=input_layer, outputs=latent, name='encoder')\n",
    "\n",
    "    # Decoder\n",
    "    latent_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    h = latent_input\n",
    "    for i, dim in enumerate(reversed(hidden_dims)):\n",
    "        h = Dense(dim, activation='relu', name=f'decoder_hidden_{i}')(h)\n",
    "    # Sigmoid activation if input X is normalized to [0,1]. Otherwise, 'linear' or 'tanh' might be better.\n",
    "    output_layer = Dense(input_dim, activation='sigmoid', name='reconstruction_output')(h)\n",
    "    decoder = Model(inputs=latent_input, outputs=output_layer, name='decoder')\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder(encoder(input_layer)), name='autoencoder')\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# --- 2. Loss Functions & Target Distribution ---\n",
    "def target_distribution(q):\n",
    "    \"\"\"\n",
    "    Computes the target distribution p from soft assignments q, by sharpening.\n",
    "    P_ij = (q_ij^2 / f_j) / sum_k(q_ik^2 / f_k), where f_j = sum_i q_ij.\n",
    "    \"\"\"\n",
    "    weight = q ** 2 / K.sum(q, axis=0)\n",
    "    return K.transpose(K.transpose(weight) / K.sum(weight, axis=1))\n",
    "\n",
    "# --- 3. Training Logic ---\n",
    "def train_deep_joint_clustering(X, y=None, n_clusters=10,\n",
    "                                latent_dim=10, ae_hidden_dims=[500, 500, 2000],\n",
    "                                ae_epochs=50, ae_batch_size=256,\n",
    "                                joint_epochs=100, joint_batch_size=256, # joint_batch_size for model.fit\n",
    "                                update_interval_epochs=10, # How often (in epochs) to update target P\n",
    "                                gamma=0.1, # Coefficient for clustering loss\n",
    "                                tol=1e-3, # Tolerance for stopping criterion based on label changes\n",
    "                                random_seed=42):\n",
    "    \"\"\"Trains the IDEC-like deep joint clustering model.\"\"\"\n",
    "    tf.random.set_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Phase 1: Autoencoder Pre-training\n",
    "    print(\"--- Phase 1: Autoencoder Pre-training ---\")\n",
    "    autoencoder, encoder, decoder = build_autoencoder(input_dim, latent_dim, ae_hidden_dims)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    autoencoder.fit(X, X, batch_size=ae_batch_size, epochs=ae_epochs, verbose=1)\n",
    "    print(\"Autoencoder pre-training finished.\")\n",
    "\n",
    "    # Phase 2: Initialize Cluster Centroids\n",
    "    print(\"\\n--- Phase 2: Initializing Cluster Centroids ---\")\n",
    "    latent_representations = encoder.predict(X)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=random_seed) # n_init='auto' from scikit-learn 1.4\n",
    "    y_pred_kmeans = kmeans.fit_predict(latent_representations)\n",
    "    cluster_centers_init = kmeans.cluster_centers_\n",
    "    print(\"Cluster centroids initialized using K-means on latent space.\")\n",
    "    if y is not None:\n",
    "        print(f\"K-means on latent NMI: {normalized_mutual_info_score(y, y_pred_kmeans):.4f}, \"\n",
    "              f\"ARI: {adjusted_rand_score(y, y_pred_kmeans):.4f}\")\n",
    "\n",
    "    # Phase 3: Joint Training\n",
    "    print(\"\\n--- Phase 3: Joint Clustering and Representation Learning ---\")\n",
    "    # Build the full IDEC model for joint training\n",
    "    model_input = Input(shape=(input_dim,), name='idec_input')\n",
    "    latent_z = encoder(model_input) # Use the pre-trained encoder\n",
    "    q_output = ClusteringLayer(n_clusters, weights=[cluster_centers_init], name='clustering')(latent_z)\n",
    "    reconstruction_output = decoder(latent_z) # Use the pre-trained decoder\n",
    "\n",
    "    idec_model = Model(inputs=model_input, outputs=[q_output, reconstruction_output], name='idec_model')\n",
    "    # The output names 'clustering' and 'decoder' (name of the Model) are used for loss keys.\n",
    "    # idec_model.summary() # Useful for debugging output names.\n",
    "\n",
    "    idec_model.compile(optimizer=Adam(learning_rate=0.001), # Can use a smaller LR for fine-tuning\n",
    "                       loss={'clustering': 'kld',  # Kullback-Leibler divergence for clustering output\n",
    "                             'decoder': 'mse'},    # Mean Squared Error for reconstruction output (decoder Model)\n",
    "                       loss_weights={'clustering': gamma, 'decoder': 1.0})\n",
    "\n",
    "    y_pred_last = y_pred_kmeans\n",
    "    p_target = None\n",
    "\n",
    "    for epoch in range(joint_epochs):\n",
    "        if epoch % update_interval_epochs == 0:\n",
    "            print(f\"\\nEpoch {epoch}/{joint_epochs}\")\n",
    "            q_pred_current, _ = idec_model.predict(X, verbose=0)\n",
    "            p_target = target_distribution(tf.convert_to_tensor(q_pred_current)).numpy() # Update target P\n",
    "\n",
    "            y_pred_current = np.argmax(q_pred_current, axis=1)\n",
    "            if y is not None:\n",
    "                nmi = normalized_mutual_info_score(y, y_pred_current)\n",
    "                ari = adjusted_rand_score(y, y_pred_current)\n",
    "                print(f\"NMI = {nmi:.4f}, ARI = {ari:.4f}\")\n",
    "\n",
    "            # Check for convergence: if label assignments change less than tol\n",
    "            delta_label = np.sum(y_pred_current != y_pred_last).astype(np.float32) / y_pred_current.shape[0]\n",
    "            y_pred_last = y_pred_current\n",
    "            if epoch > 0 and delta_label < tol:\n",
    "                print(f\"Reached label change tolerance ({delta_label:.4f} < {tol}). Stopping training.\")\n",
    "                break\n",
    "        \n",
    "        if p_target is None: # For the very first iteration if update_interval > 0\n",
    "             q_pred_init, _ = idec_model.predict(X, verbose=0)\n",
    "             p_target = target_distribution(tf.convert_to_tensor(q_pred_init)).numpy()\n",
    "\n",
    "        # Train for one epoch using model.fit.\n",
    "        # Note: X is used for both reconstruction target and input.\n",
    "        # p_target is the target for the clustering head.\n",
    "        history = idec_model.fit(X, {'clustering': p_target, 'decoder': X},\n",
    "                                 batch_size=joint_batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "        if epoch % update_interval_epochs == 0 :\n",
    "            total_loss = history.history['loss'][0]\n",
    "            clus_loss = history.history['clustering_loss'][0]\n",
    "            recon_loss = history.history['decoder_loss'][0] # Ensure key matches with model output name\n",
    "            print(f\"Total Loss: {total_loss:.4f}, Clustering Loss: {clus_loss:.4f}, Recon Loss: {recon_loss:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"Joint training finished.\")\n",
    "    q_final, _ = idec_model.predict(X, verbose=0)\n",
    "    y_pred_final = np.argmax(q_final, axis=1)\n",
    "\n",
    "    return y_pred_final, idec_model\n",
    "\n",
    "# --- 4. Image Loading and Preprocessing Function ---\n",
    "def load_and_preprocess_images(image_dir, target_size=(64, 64)):\n",
    "    pil_images = []\n",
    "    image_filenames = []\n",
    "    processed_images_list = []\n",
    "\n",
    "    print(f\"Loading images from: {image_dir}\")\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\")\n",
    "    \n",
    "    if not os.path.isdir(image_dir):\n",
    "        print(f\"Error: Directory not found: {image_dir}\")\n",
    "        return None, None, None\n",
    "\n",
    "    for filename in sorted(os.listdir(image_dir)):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            try:\n",
    "                img_path = os.path.join(image_dir, filename)\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Convert to RGB (handles grayscale, RGBA, etc.)\n",
    "                img = img.convert('RGB')\n",
    "                \n",
    "                # Resize\n",
    "                img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                pil_images.append(img_resized) # Store resized PIL image if needed later\n",
    "                image_filenames.append(filename)\n",
    "                \n",
    "                # Convert to NumPy array and normalize to [0,1]\n",
    "                img_array = np.array(img_resized) / 255.0\n",
    "                processed_images_list.append(img_array)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load/process image {filename}. Error: {e}\")\n",
    "    \n",
    "    if not processed_images_list:\n",
    "        print(\"No images were successfully processed.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"{len(processed_images_list)} images loaded and preprocessed successfully.\")\n",
    "    \n",
    "    # Stack into a single NumPy array\n",
    "    X_images_raw = np.stack(processed_images_list, axis=0)\n",
    "    \n",
    "    # Flatten images for MLP autoencoder: (num_images, height * width * channels)\n",
    "    num_images, height, width, channels = X_images_raw.shape\n",
    "    X_images_flattened = X_images_raw.reshape(num_images, height * width * channels)\n",
    "    \n",
    "    current_input_dim = X_images_flattened.shape[1]\n",
    "    print(f\"Flattened image data shape: {X_images_flattened.shape}\")\n",
    "    \n",
    "    return X_images_flattened, image_filenames, current_input_dim\n",
    "\n",
    "\n",
    "# --- 5. Example Usage with Your Image Data ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters for your image data ---\n",
    "    image_dir = \"./storage/clean/blood_cell/segmenter\" # YOUR IMAGE DIRECTORY\n",
    "    \n",
    "    # User-specified parameters:\n",
    "    TARGET_IMAGE_SIZE = (360, 360) # Resize images to this (height, width)\n",
    "    N_CLUSTERS_IMAGES = 5          # Number of clusters\n",
    "    LATENT_DIM_IMAGES = 128        # Latent dimension for image features (encoding dim)\n",
    "\n",
    "    # Derived input dimension: height * width * channels (assuming 3 for RGB)\n",
    "    # This will be calculated in load_and_preprocess_images, but good to be aware:\n",
    "    # INPUT_DIM_CALCULATED = TARGET_IMAGE_SIZE[0] * TARGET_IMAGE_SIZE[1] * 3\n",
    "    # print(f\"Expected input dimension: {INPUT_DIM_CALCULATED}\") # Should be 388800\n",
    "\n",
    "    # MLP AE hidden layers for images.\n",
    "    # For INPUT_DIM = 388800 and LATENT_DIM = 128, this is a very large reduction.\n",
    "    # Consider these carefully. A CAE would be much better.\n",
    "    AE_HIDDEN_DIMS_IMAGES = [2048, 1024, 512] # Example: Adjust these layers.\n",
    "                                             # Must be less than input_dim.\n",
    "                                             # The last one should be > LATENT_DIM_IMAGES ideally.\n",
    "\n",
    "    AE_EPOCHS_IMAGES = 30      # Adjust as needed. May need more for large inputs.\n",
    "    JOINT_EPOCHS_IMAGES = 50   # Adjust as needed.\n",
    "    # WARNING: With 360x360 images, batch sizes might need to be very small\n",
    "    AE_BATCH_SIZE_IMAGES = 16  # Try small batch size due to large image dimensions\n",
    "    JOINT_BATCH_SIZE_IMAGES = 16 # Try small batch size\n",
    "    GAMMA_IMAGES = 0.1         # Weight for clustering loss\n",
    "\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! WARNING: Using MLP Autoencoder for 360x360 images is computationally !!!\")\n",
    "    print(\"!!! intensive and likely suboptimal. Consider using a Convolutional      !!!\")\n",
    "    print(\"!!! Autoencoder (CAE) for better results and efficiency with images.     !!!\")\n",
    "    print(\"!!! You may encounter memory issues or very long training times.         !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "\n",
    "    # Load and preprocess your images\n",
    "    X_images_flattened, image_filenames, input_dim_images = load_and_preprocess_images(\n",
    "        image_dir,\n",
    "        target_size=TARGET_IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    if X_images_flattened is not None:\n",
    "        print(f\"\\nStarting deep joint clustering for image data with actual input_dim: {input_dim_images}\")\n",
    "        if input_dim_images != TARGET_IMAGE_SIZE[0] * TARGET_IMAGE_SIZE[1] * 3:\n",
    "            print(f\"Warning: Calculated input_dim ({input_dim_images}) doesn't match expected from TARGET_IMAGE_SIZE * 3.\")\n",
    "            print(\"This might happen if some images were not RGB or had issues during loading.\")\n",
    "\n",
    "        y_images = None # Or load your labels if available\n",
    "\n",
    "        y_pred_final_images, trained_idec_model_images = train_deep_joint_clustering(\n",
    "            X_images_flattened,\n",
    "            y=y_images,\n",
    "            n_clusters=N_CLUSTERS_IMAGES,\n",
    "            input_dim_param=input_dim_images,\n",
    "            latent_dim=LATENT_DIM_IMAGES,\n",
    "            ae_hidden_dims=AE_HIDDEN_DIMS_IMAGES,\n",
    "            ae_epochs=AE_EPOCHS_IMAGES,\n",
    "            ae_batch_size=AE_BATCH_SIZE_IMAGES,\n",
    "            joint_epochs=JOINT_EPOCHS_IMAGES,\n",
    "            joint_batch_size=JOINT_BATCH_SIZE_IMAGES,\n",
    "            gamma=GAMMA_IMAGES,\n",
    "            tol=0.001,\n",
    "            random_seed=42\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Final Image Clustering Results ---\")\n",
    "        print(f\"Predicted cluster assignments shape: {y_pred_final_images.shape}\")\n",
    "        # You can now map y_pred_final_images back to image_filenames\n",
    "        # for fname, cluster_id in zip(image_filenames, y_pred_final_images):\n",
    "        #     print(f\"Image: {fname}, Cluster ID: {cluster_id}\") # Uncomment to see all\n",
    "\n",
    "        # Print first few and summary\n",
    "        for i in range(min(10, len(image_filenames))):\n",
    "            print(f\"Image: {image_filenames[i]}, Cluster ID: {y_pred_final_images[i]}\")\n",
    "        if len(image_filenames) > 10:\n",
    "            print(\"...\")\n",
    "        \n",
    "        unique_clusters, counts = np.unique(y_pred_final_images, return_counts=True)\n",
    "        print(\"\\nCluster distribution:\")\n",
    "        for cluster_id, count in zip(unique_clusters, counts):\n",
    "            print(f\"Cluster {cluster_id}: {count} images\")\n",
    "\n",
    "\n",
    "        if y_images is not None and len(y_images) == len(X_images_flattened):\n",
    "            final_nmi_images = normalized_mutual_info_score(y_images, y_pred_final_images)\n",
    "            final_ari_images = adjusted_rand_score(y_images, y_pred_final_images)\n",
    "            print(f\"Final NMI for images: {final_nmi_images:.4f}\")\n",
    "            print(f\"Final ARI for images: {final_ari_images:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nGround truth labels (y_images) not provided, so NMI/ARI are not calculated.\")\n",
    "    else:\n",
    "        print(\"Exiting due to image loading/processing issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
